import argparse
import logging
import os
import time
import random
from pycocotools.coco import COCO
import pycocotools.mask as mask_utils
import torch
import cv2
import json
import pycocotools
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt

from itertools import groupby
from scipy.ndimage import convolve
from tqdm import tqdm
from os.path import join
from PIL import Image, ImageDraw, ImageOps
from transformers import pipeline

from scripts.sd_inpaint_dreambooth import StableDiffusionModel as DreamBoothPipe
from scripts.stable_diffusion import StableDiffusionModel as SDPipe
from scripts.depth.depth_estimator import DepthEstimator, DEPTH_MODELS 
from scripts.controlnet_sd import AugmentationPipe as ControlNetPipe


class AugmentDataset():
    def __init__(self, args) -> None:
        self.args = args

        self.prompts_path = None
        self.images_path = None
        self.annotation_path = None
        self.output_path = None
        self.model_checkpoint = None
        self.masks_path = None
        self.padding = None
        self.iter_number = None
        self.guidance_scale = None
        self.lora_chkpt = None

        self.diffusion_pipe = None
        self.depth_pipe = None
        self.annotation = None
        self.reference_annotation = None
        self.prompts = None
        self.scene_prompts = None
        self.logger = None
        self.epsilon = 0.5

        self.setup()        

    def setup(self):
        self.parse_args(self.args)
        self.set_seed()

        self.make_dirs(self.output_path)
        # self.make_dirs(f"{self.output_path}/images")

        self.logger = logging.getLogger(__name__)
        path = os.path.join("log.log")
        # path = os.path.join(self.output_path, "log.log")
        logging.basicConfig(filename=path, level=logging.INFO)

        self.load_pipes()
        self.load_prompts()

        self.init_annotation()

    def set_seed(self, ):
        seed = int(self.seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        random.seed(seed)

    def parse_args(self, args):
        self.prompts_path = args.prompts_path
        self.scene_prompts_path = args.scene_prompts_path
        self.images_path = args.images_path
        self.annotation_path = args.annotation_path
        self.output_path = args.output_path
        self.masks_path = args.masks_path
        self.padding = args.padding
        self.iter_number = args.iter_number
        self.guidance_scale = args.guidance_scale
        self.lora_chkpt = args.lora_chkpt
        self.device = args.device
        self.seed = args.seed
        self.depth_model = args.depth_model
        self.synth_scenes = args.synth_scenes

        if args.sd_chkpt:
            self.model_checkpoint = args.sd_chkpt
            self.pipe = SDPipe

        elif args.dreambooth_chkpt:
            self.model_checkpoint = args.dreambooth_chkpt
            self.pipe = DreamBoothPipe

        else:
            raise ValueError("Neither --sd_chkpt nor --dreambooth_chkpt were provided.")

    def make_dirs(self, path):
        if not os.path.exists(path):
            os.makedirs(path)

    def init_annotation(self):
        self.annotation =  {
            "info": {
                "description": "Dataset generated by augmentation",
                "url": "",
                "version": "",
                "year": "",
                "contributor": "",
                "date_created": ""
            },
            "licenses": [
                {
                    "id": int(0),
                    "name": "",
                    "url": ""
                }
            ],
            "images": [],
            "categories": [],
            "annotations": []
        }

        for idx, prompt in enumerate(self.prompts):
            self.annotation["categories"].append({
                "id": int(idx+1),
                "name": prompt,
                "supercategory": ""
            })

    def load_mask(self, masks_path):
        mask = Image.open(masks_path).convert('L')
        if random.random() < self.epsilon:
            mask, angle = self.random_rotate(mask)
            self.logger.info(f"Rotate mask angle: {angle}")
        object_width, object_height, cropped_object = self.get_masked_object(mask)
        self.logger.info(f"Find object dimensions: {object_width}x{object_height}")

        return mask, object_width, object_height, cropped_object

    def load_pipes(self):
        self.diffusion_pipe = self.pipe(pretrained=self.model_checkpoint, device=self.device)
        self.logger.info(f"Load pipeline: {self.pipe.__name__}")

        if self.lora_chkpt:
            self.diffusion_pipe.load_lora(self.lora_chkpt)

        args = argparse.Namespace(
            model=self.depth_model,
            calc_metrics=False,
            device=self.device,
            images_dir=None,
            output_path=None,
        )

        self.depth_pipe = DepthEstimator(args)
        self.logger.info("Load DEPTH pipeline")

        if self.synth_scenes:
            self.scene_generation_pipe = ControlNetPipe(device=self.device, seed=self.seed)
            self.logger.info("Load ControlNet pipeline")

    def load_prompts(self):
        with open(self.prompts_path, "r") as file:
            self.prompts = [line.rstrip() for line in file]

        self.logger.info("Load prompts: "+self.prompts_path)

        if self.synth_scenes:
            with open(self.scene_prompts_path, "r") as file:
                self.scene_prompt = [line.rstrip() for line in file]
                # TODO: fix this shit
                self.scene_prompts = self.scene_prompt[0]

            self.logger.info("Load scene prompts: "+self.scene_prompts_path)

    def load_annotation(self):
        if isinstance(self.annotation_path, str):
            self.annotation_path = Path(self.annotation_path)
        
        images_paths = sorted([os.path.join(self.images_path, f) for f in 
                               os.listdir(self.images_path)])

        if self.annotation_path.suffix == '.json':
            self.reference_annotation = COCO(self.annotation_path)
            background_masks_paths = self.reference_annotation.dataset['annotations']

        elif self.annotation_path.is_dir():
            background_masks_paths = sorted([os.path.join(self.annotation_path, f) for f 
                                             in os.listdir(self.annotation_path)])

        return images_paths, background_masks_paths

    def get_annotation(self, image_pth, mask_path):
        image = Image.open(image_pth)
        self.logger.info(f"Load image: {image_pth}")

        if isinstance(mask_path, dict):
            img_name = image_pth.split('/')[-1]
            img_id = next((img['id'] for img in self.reference_annotation.dataset["images"] 
                if img['file_name'] == img_name), None)
            # works only for one annotation per image
            mask_ids = self.reference_annotation.getAnnIds(imgIds=[img_id])
            mask_annot = self.reference_annotation.loadAnns(ids=mask_ids)[0]
            img_info = self.reference_annotation.loadImgs(ids=[img_id])[0]
            rle = mask_utils.frPyObjects(mask_annot["segmentation"], 
                                         img_info["width"],
                                         img_info["height"],)
                                        #  mask_annot["segmentation"]["size"][0], 
                                        #  mask_annot["segmentation"]["size"][1])
            background_mask = mask_utils.decode(rle)*255
            background_mask = np.squeeze(background_mask.astype('uint8'))
            background_mask = Image.fromarray(background_mask)
            self.logger.info(f"Load background mask {mask_annot['id']}")

        if isinstance(mask_path, str):
            background_mask = Image.open(mask_path)
            self.logger.info(f"Load background mask: {mask_path}")

            filename_img = self.get_filename_without_ext(image_pth)
            filename_background_mask = self.get_filename_without_ext(mask_path)
            assert filename_img.split(".")[0] == filename_background_mask.split(".")[0]
        
        return image, background_mask

    def get_masked_object(self, image):
        image_array = np.array(image)

        object_mask = (image_array > 200)
        assert object_mask.shape != ()
        object_y, object_x = np.where(object_mask)

        min_x, max_x = object_x.min(), object_x.max()
        min_y, max_y = object_y.min(), object_y.max()
        width = max_x - min_x + self.padding
        height = max_y - min_y + self.padding

        cropped_image = image.crop((min_x, min_y, max_x, max_y))
        cropped_image = ImageOps.expand(cropped_image, border=self.padding, fill='black')

        return width, height, cropped_image
    
    def get_filename_without_ext(self, image_pth):
        base_name = os.path.basename(image_pth)  # Get the filename
        name_without_ext = os.path.splitext(base_name)[0]  # Remove the last extension
        return name_without_ext

    def get_valid_coordinates(self, background_mask, object_height, object_width, depth_map):
        background_mask = np.array(background_mask)
        background_object = np.where(background_mask == 255)
        background_object_coords = list(zip(background_object[1], background_object[0]))

        valid_coordinates = []
        for coords in background_object_coords:
            x, y = coords
            resize_coeff = np.mean(depth_map[y:y+object_height, x:x+object_width])
            target_height = int(object_height * resize_coeff)
            target_width = int(object_width * resize_coeff)

            if y+target_height < background_mask.shape[0] and x+target_width < background_mask.shape[1]:
                if np.all(background_mask[y:y+target_height, x:x+target_width] == 255):
                        valid_coordinates.append((x, y, resize_coeff))


        return valid_coordinates, background_object_coords

    def random_rotate(self, mask):
        angle = random.randint(-360, 360)
        rotated_mask = mask.rotate(angle)
        return rotated_mask, angle
    
    def get_segmentation_rle(self, mask):
        mask = np.array(mask)
        mask = mask.astype(np.uint8)
    
        mask_binary = (mask > 0).astype(np.uint8)
        mask_fortran = np.asfortranarray(mask_binary)
        rle = mask_utils.encode(mask_fortran)
        rle = {
            "counts": rle["counts"].decode("utf-8"),
            "size": [int(mask.shape[0]), int(mask.shape[1])]
        }
        area = mask_utils.area(rle)

        return rle, area
    
    def add_image_info(self, new_image_name, image):
        # add image info to annotation
        next_id = len(self.annotation["images"]) + 1
        self.annotation["images"].append({
            "id": int(next_id),
            "file_name": new_image_name,
            "width": int(image.size[0]),
            "height": int(image.size[1]),
            "coco_url": "",
            "flickr_url": "",
            "license": int(0),
            "date_captured": "",
            "coco_url": ""
        })

    def add_annotation(self, img_id,
                       image, cropped_resized_object, 
                       random_x, random_y, 
                       box, prompt):

        category_id = 1
        # category_id = [v for v in self.annotation["categories"] if v["name"] == prompt][0]["id"]
        mask_image = Image.new("L", (image.size[0], image.size[1]), 0)
        mask_image.paste(cropped_resized_object, (random_x, random_y))
                
        rle, area = self.get_segmentation_rle(mask_image)

        next_id = len(self.annotation["annotations"]) + 1
        self.annotation["annotations"].append({
            "id": int(next_id),
            "image_id": int(img_id),
            "category_id": int(category_id),
            "segmentation": rle,
            "area": int(area),
            "bbox": box,
            "iscrowd": int(0)
        })
    
    def prepare_depth(self, depth):
        depth = np.asarray(depth)
        return (depth - depth.min()) / (depth.max() - depth.min()) 

    # DEBUG
    def draw_point_and_save(self, mask, random_coordinates, save_path):
        image = mask.convert('RGB')
        draw = ImageDraw.Draw(image)

        left = random_coordinates[0] - 5
        top = random_coordinates[1] - 5
        right = random_coordinates[0] + 5
        bottom = random_coordinates[1] + 5

        draw.ellipse([(left, top), (right, bottom)], outline='red')
        image.save(save_path)

    def draw_valid_coordinates_and_save(self, mask, valid_coordinates, save_path):
        image = mask.convert('RGB')
        draw = ImageDraw.Draw(image)

        for coord in valid_coordinates:
            draw.point([coord[0], coord[1]], fill='red')

        image.save(save_path)

    def save_depth(self, depth, save_path):
        depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0
        depth = depth.astype(np.uint8)
        depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)
        cv2.imwrite(save_path, depth)

    # MAIN FUNCTIONS
    def generate_scene(self, estimator_ouput, 
                       guidance_scale=3, controlnet_conditioning_scale=0.7):
        return self.scene_generation_pipe(prompt=self.scene_prompts, 
                                        estimator_ouput=estimator_ouput, 
                                        guidance_scale=guidance_scale, 
                                        controlnet_conditioning_scale=controlnet_conditioning_scale)

    def generate_object(self, image, background_mask, 
                        object_height, object_width, 
                        depth_map, cropped_object, prompt):
        
        depth_map = self.prepare_depth(depth_map)
        valid_coordinates, background_object_coords = self.get_valid_coordinates(background_mask, object_height, object_width, depth_map)

        # Randomly select one of the valid coordinates
        if valid_coordinates == []:
            self.logger.info(f"No valid coordinates found")
            return None, None, None, None, None

        random_coordinates = random.choice(valid_coordinates)
        random_x, random_y, resize_coeff = random_coordinates
        self.logger.info(f"Random coordinates: {random_coordinates}")

        # self.draw_point_and_save(background_mask, random_coordinates, f"{self.output_path}/{self.filename_img}_background_mask.png")
        # self.draw_valid_coordinates_and_save(background_mask, valid_coordinates, f"{self.output_path}/{self.filename_img}_valid_coordinates.png")
        # self.draw_valid_coordinates_and_save(background_mask, background_object_coords, f"{self.output_path}/{self.filename_img}_background_object_coords.png")
        self.save_depth(depth_map, f"{self.output_path}/{self.filename_img}/depth.png")

        # resize mask due to depth map
        resized_width = int(object_width * resize_coeff)
        resized_height = int(object_height * resize_coeff)
        cropped_resized_object = cropped_object.resize((resized_width, resized_height))
        
        # get box with mask (log to file [x, y, w, h])
        box = [int(random_x), int(random_y), int(resized_width), int(resized_height)]
        self.logger.info(f"Box: {box}")
        cropped_image = image.crop((random_x, random_y, random_x+resized_width, random_y+resized_height))
        detailed_prompt = f"A highly detailed and realistic depiction of {prompt}, featuring sharp details, natural lighting, and a clean, well-balanced composition."
        
        # inpainting
        start_generating_time = time.time()
        augmented_image = self.diffusion_pipe(
            cropped_image, cropped_resized_object, 
            detailed_prompt, None, image.size[0], image.size[1],
            self.iter_number, self.guidance_scale
        )

        generation_time = time.time() - start_generating_time
        self.avg_generation_time += generation_time

        # debug
        augmented_image.save(f"{self.output_path}/{self.filename_img}/augmented_image_{prompt}.png")
        
        self.logger.info(f"Prompt: {prompt}")
        self.logger.info(f"Generation time: {generation_time}")

        resized_augmented_image = augmented_image.resize((resized_width, resized_height))

        augmented_image = image.copy()
        augmented_image.paste(resized_augmented_image, (random_x, random_y))

        augmented_mask = background_mask.copy()
        augmented_mask.paste(cropped_resized_object, (random_x, random_y))  

        return augmented_image, cropped_resized_object, random_x, random_y, box
    
    def augment_dataset(self):
        self.logger.info("Start generating")
        avg_total_time = 0
        self.avg_generation_time = 0
        img_id = 1

        start_total_time = time.time()

        images_paths, background_masks_paths = self.load_annotation()
        total_count_of_imgs = len(images_paths)
        self.target_masks = sorted([join(self.masks_path, f) for f in os.listdir(self.masks_path)])

        for idx, image_pth in tqdm(enumerate(images_paths)):
            start_time = time.time()
            print(f"Start generating {idx+1}/{total_count_of_imgs}")

            self.filename_img = "".join(image_pth.split('/')[-1].split('.')[:-1])
            self.make_dirs(f"{self.output_path}/{self.filename_img}")

            # load image and its background mask 
            image, background_mask = self.get_annotation(image_pth, background_masks_paths[idx])

            depth_map = self.depth_pipe.calculate_depth(image)

            # get random pseudo mask (NOW USE ONLY ONE MASK)
            random_mask_path = random.choice(self.target_masks)
            self.logger.info(f"Load random mask: {random_mask_path}")
            target_mask, object_width, object_height, cropped_object = self.load_mask(random_mask_path)

            # generate scene 
            if self.synth_scenes:
                scene = self.generate_scene(depth_map)
            else:
                scene = image 
            scene.save(f"{self.output_path}/{self.filename_img}/scene.png")

            # generate object
            for i, prompt in enumerate(self.prompts):
                augmented_image, cropped_resized_object, \
                    random_x, random_y, box = self.generate_object(
                        scene, background_mask, 
                        object_height, object_width, 
                        depth_map, cropped_object, prompt)

                if not augmented_image:
                    self.add_image_info(f"{self.filename_img}_{prompt}.png", scene)
                    img_id += 1
                    self.logger.info(f"GENERATION ERROR")
                    continue

                new_image_name = f"{self.filename_img}_{prompt}.png"
                augmented_image.save(f"{self.output_path}/{self.filename_img}/"+new_image_name)
                # augmented_image.save(f"{self.output_path}/images/"+new_image_name)

                self.add_image_info(new_image_name, augmented_image)

                img_id = len(self.annotation["images"])
                self.add_annotation(img_id, scene, cropped_resized_object, 
                                    random_x, random_y, 
                                    box, "pothole")
                                    # box, prompt)

            avg_total_time += time.time() - start_time
            self.logger.info(f"Average generation time: {self.avg_generation_time/(idx+1)}")

        with open(f"{self.output_path}/annotation.json", 'w') as fp:
            (json.dump(self.annotation, fp))

        total_time = time.time() - start_total_time
        self.logger.info(f"Total time: {total_time}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--images_path", type=str, required=True)
    parser.add_argument("--annotation_path", type=str, required=True)
    parser.add_argument("--prompts_path", type=str, required=True)
    parser.add_argument("--masks_path", type=str, required=True)
    parser.add_argument("--output_path", type=str, required=True)
    parser.add_argument("--depth_model", type=str, choices=DEPTH_MODELS.keys(), default=DEPTH_MODELS["Depth_Anything_v2"])
    parser.add_argument("--sd_chkpt", type=str)
    parser.add_argument("--dreambooth_chkpt", type=str)
    parser.add_argument("--lora_chkpt", type=str)
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--padding", type=int, default="0")
    parser.add_argument("--iter_number", type=int, default="20")
    parser.add_argument("--guidance_scale", type=float, default="0.7")
    parser.add_argument("--seed", type=float, default="0")
    parser.add_argument("--synth_scenes", type=bool, default=False)
    parser.add_argument("--scene_prompts_path", type=str, default="Realistic image",
                        help="If --synth_scenes=True, use this arg to customize scene generation")

    args = parser.parse_args()
    
    dataset_augmentator = AugmentDataset(args)
    dataset_augmentator.augment_dataset()